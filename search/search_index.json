{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project mission Helpful AI for Home and Business Automation Ambianic is an Open Source AI platform that puts local control and privacy first. It enables users to run and train custom AI models without compromising privacy and without sharing data with cloud providers. View on Github Why? What's the root cause for Ambianic.ai to exist? Below is a 5 Whys diagram that tries to answer this key question objectively. Needless to say there are subjective reasons which are equally if not more influential for the existence of this project such as basic human excitement to serve a bigger purpose via open source AI. User Journey Ambianic's roadmap is inspired by user stories and community feedback. The following diagram illustrates an example user journey. User journeys help us align on the bigger picture and segue into agile development constructs such as user stories and sprints. More user journeys will be added over time as the project evolves. Some of the candidate topics include: Home Automation Turn traditional door locks into smart locks with Face Recognition. Alert parents if a crying toddler is left unattended for more than 15 minutes. Raise an alert if a baby is seated near a car door without child lock enabled while in motion. Business Automation Prevent accidents by alerting drivers who act sleepy or distracted. Make sure that a factory floor position is not left unattended for more than 15 minutes. Recognize presence of unauthorized people in a restricted access work area. User - System Interactions Users interact with the system in two phases: First Time Installation Consequent App Engagements The following diagram illustrates the high level user - system interactions. User Interface Flow The User Interface is centered around three main activities: Setup Ambianic Edge to communicate with smart home devices: sensors, cameras, microphones, lights, door locks, and others. Design flows to automatically observe sensors and make helpful recommendations. Review event timeline, alerts and actionable recommendations. The Ambianic UI is architected as an Offline-First PWA (Progressive Web Application). It does not assume that the user has constant broadband internet access when she needs to interact with Ambianic local data and devices. We anticipate a range of real world scenarios with low bandwidth or no-Internet access at all when the use may need to review Ambianic alerts, timeline data, edit flows and configure Edge devices. It stores data locally on the client device (mobile or desktop) and, when there\u2019s a network connection, syncs data to the user's Ambianic server and resolves any data conflicts. When possible it communicates directly with local Ambianic Edge devices minimizing network routing overhead. Project Status At this time, Ambianic is in active early formation stages. Design and implementation decisions are made daily with focus on advancing the project to an initial stable version as soon as possible. If you are willing to take the risk that comes with early stage code and are able to dive deep into Python, Javascript, Gstreamer, and Tensorflow code, then please keep reading. Otherwise you can register to watch for new releases . We will notify you as soon as a stable release is out. Product Design Goals Our goal is to build a product that is useful out of the box: Less than 15 minutes setup time Less than $75 in hardware costs Primary platform: Raspberry Pi 4 B, 4GB RAM, 32GB SDRAM No coding required to get started Decomposable and hackable for open source developers Run in Development Mode If you would like to try the current development version, follow these steps: Clone this git repository. ./ambianic-start.sh Study config.yaml and go from there. Contributors If you are interested in becoming a contributor to the project, please read the Contributing page and follow the steps. Looking forward to hearing from you!","title":"Home"},{"location":"#project-mission","text":"","title":"Project mission"},{"location":"#helpful-ai-for-home-and-business-automation","text":"Ambianic is an Open Source AI platform that puts local control and privacy first. It enables users to run and train custom AI models without compromising privacy and without sharing data with cloud providers. View on Github","title":"Helpful AI for Home and Business Automation"},{"location":"#why","text":"What's the root cause for Ambianic.ai to exist? Below is a 5 Whys diagram that tries to answer this key question objectively. Needless to say there are subjective reasons which are equally if not more influential for the existence of this project such as basic human excitement to serve a bigger purpose via open source AI.","title":"Why?"},{"location":"#user-journey","text":"Ambianic's roadmap is inspired by user stories and community feedback. The following diagram illustrates an example user journey. User journeys help us align on the bigger picture and segue into agile development constructs such as user stories and sprints. More user journeys will be added over time as the project evolves. Some of the candidate topics include: Home Automation Turn traditional door locks into smart locks with Face Recognition. Alert parents if a crying toddler is left unattended for more than 15 minutes. Raise an alert if a baby is seated near a car door without child lock enabled while in motion. Business Automation Prevent accidents by alerting drivers who act sleepy or distracted. Make sure that a factory floor position is not left unattended for more than 15 minutes. Recognize presence of unauthorized people in a restricted access work area.","title":"User Journey"},{"location":"#user-system-interactions","text":"Users interact with the system in two phases: First Time Installation Consequent App Engagements The following diagram illustrates the high level user - system interactions.","title":"User - System Interactions"},{"location":"#user-interface-flow","text":"The User Interface is centered around three main activities: Setup Ambianic Edge to communicate with smart home devices: sensors, cameras, microphones, lights, door locks, and others. Design flows to automatically observe sensors and make helpful recommendations. Review event timeline, alerts and actionable recommendations. The Ambianic UI is architected as an Offline-First PWA (Progressive Web Application). It does not assume that the user has constant broadband internet access when she needs to interact with Ambianic local data and devices. We anticipate a range of real world scenarios with low bandwidth or no-Internet access at all when the use may need to review Ambianic alerts, timeline data, edit flows and configure Edge devices. It stores data locally on the client device (mobile or desktop) and, when there\u2019s a network connection, syncs data to the user's Ambianic server and resolves any data conflicts. When possible it communicates directly with local Ambianic Edge devices minimizing network routing overhead.","title":"User Interface Flow"},{"location":"#project-status","text":"At this time, Ambianic is in active early formation stages. Design and implementation decisions are made daily with focus on advancing the project to an initial stable version as soon as possible. If you are willing to take the risk that comes with early stage code and are able to dive deep into Python, Javascript, Gstreamer, and Tensorflow code, then please keep reading. Otherwise you can register to watch for new releases . We will notify you as soon as a stable release is out.","title":"Project Status"},{"location":"#product-design-goals","text":"Our goal is to build a product that is useful out of the box: Less than 15 minutes setup time Less than $75 in hardware costs Primary platform: Raspberry Pi 4 B, 4GB RAM, 32GB SDRAM No coding required to get started Decomposable and hackable for open source developers","title":"Product Design Goals"},{"location":"#run-in-development-mode","text":"If you would like to try the current development version, follow these steps: Clone this git repository. ./ambianic-start.sh Study config.yaml and go from there.","title":"Run in Development Mode"},{"location":"#contributors","text":"If you are interested in becoming a contributor to the project, please read the Contributing page and follow the steps. Looking forward to hearing from you!","title":"Contributors"},{"location":"raw-python-api/","text":"ambianic ambianic.pipeline Main module for Ambianic AI pipelines. PipeElement PipeElement(self) The basic building block of an Ambianic pipeline. healthcheck PipeElement.healthcheck(self) Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally). heartbeat PipeElement.heartbeat(self) Set the heartbeat timestamp to time.monotonic(). stop PipeElement.stop(self) Receive stop signal and act accordingly. Subclasses should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing. connect_to_next_element PipeElement.connect_to_next_element(self, next_element=None) Connect this element to the next element in the pipe. Subclasses should not have to override this method. receive_next_sample PipeElement.receive_next_sample(self, **sample) Receive next sample from a connected previous element. Subclasses should not have to override this method. :Parameters: **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. process_sample PipeElement.process_sample(self, **sample) Implement processing in subclass as a generator function. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline. :Parameters: **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: processed_sample: dict Processed sample that will be passed to the next pipeline element. HealthChecker HealthChecker(self, health_status_callback=None) Monitor overall pipeline throughput health. Attaches at the end of a pipeline to monitor its health status based on received output samples and their frequency. process_sample HealthChecker.process_sample(self, **sample) Call health callback and pass on sample as is. ambianic.pipeline.ai ambianic.pipeline.avsource ambianic.webapp","title":"ambianic"},{"location":"raw-python-api/#ambianic","text":"","title":"ambianic"},{"location":"raw-python-api/#ambianicpipeline","text":"Main module for Ambianic AI pipelines.","title":"ambianic.pipeline"},{"location":"raw-python-api/#pipeelement","text":"PipeElement(self) The basic building block of an Ambianic pipeline.","title":"PipeElement"},{"location":"raw-python-api/#healthcheck","text":"PipeElement.healthcheck(self) Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally).","title":"healthcheck"},{"location":"raw-python-api/#heartbeat","text":"PipeElement.heartbeat(self) Set the heartbeat timestamp to time.monotonic().","title":"heartbeat"},{"location":"raw-python-api/#stop","text":"PipeElement.stop(self) Receive stop signal and act accordingly. Subclasses should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing.","title":"stop"},{"location":"raw-python-api/#connect_to_next_element","text":"PipeElement.connect_to_next_element(self, next_element=None) Connect this element to the next element in the pipe. Subclasses should not have to override this method.","title":"connect_to_next_element"},{"location":"raw-python-api/#receive_next_sample","text":"PipeElement.receive_next_sample(self, **sample) Receive next sample from a connected previous element. Subclasses should not have to override this method.","title":"receive_next_sample"},{"location":"raw-python-api/#parameters","text":"**sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements.","title":":Parameters:"},{"location":"raw-python-api/#process_sample","text":"PipeElement.process_sample(self, **sample) Implement processing in subclass as a generator function. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline.","title":"process_sample"},{"location":"raw-python-api/#parameters_1","text":"**sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: processed_sample: dict Processed sample that will be passed to the next pipeline element.","title":":Parameters:"},{"location":"raw-python-api/#healthchecker","text":"HealthChecker(self, health_status_callback=None) Monitor overall pipeline throughput health. Attaches at the end of a pipeline to monitor its health status based on received output samples and their frequency.","title":"HealthChecker"},{"location":"raw-python-api/#process_sample_1","text":"HealthChecker.process_sample(self, **sample) Call health callback and pass on sample as is.","title":"process_sample"},{"location":"raw-python-api/#ambianicpipelineai","text":"","title":"ambianic.pipeline.ai"},{"location":"raw-python-api/#ambianicpipelineavsource","text":"","title":"ambianic.pipeline.avsource"},{"location":"raw-python-api/#ambianicwebapp","text":"","title":"ambianic.webapp"},{"location":"developers/api-overview/","text":"API Overview REST Backend Python Frontend JS Other","title":"API Overview"},{"location":"developers/api-overview/#api-overview","text":"REST Backend Python Frontend JS Other","title":"API Overview"},{"location":"developers/api/","text":"Ambianic APIs API Overview More about our API architecture REST API More about REST API WebSockets API More about our WebSockets API MQTT API More about our MQTT API Backend Python API More about our Python API Frontend JavaScript API More about our JavaScript API","title":"APIs"},{"location":"developers/api/#ambianic-apis","text":"","title":"Ambianic APIs"},{"location":"developers/api/#api-overview","text":"More about our API architecture","title":"API Overview"},{"location":"developers/api/#rest-api","text":"More about REST API","title":"REST API"},{"location":"developers/api/#websockets-api","text":"More about our WebSockets API","title":"WebSockets API"},{"location":"developers/api/#mqtt-api","text":"More about our MQTT API","title":"MQTT API"},{"location":"developers/api/#backend-python-api","text":"More about our Python API","title":"Backend Python API"},{"location":"developers/api/#frontend-javascript-api","text":"More about our JavaScript API","title":"Frontend JavaScript API"},{"location":"developers/architecture/","text":"Ambianic High Level Architecture Pipelines Pipe Elements Sources Outputs Connecting Pipelines Integrations","title":"Architecture"},{"location":"developers/architecture/#ambianic-high-level-architecture","text":"Pipelines Pipe Elements Sources Outputs Connecting Pipelines Integrations","title":"Ambianic High Level Architecture"},{"location":"developers/cloud-deployment/","text":"Cloud Deployment Scenarios Private Cloud Public Cloud","title":"Cloud Deployment Scenarios"},{"location":"developers/cloud-deployment/#cloud-deployment-scenarios","text":"Private Cloud Public Cloud","title":"Cloud Deployment Scenarios"},{"location":"developers/deployment/","text":"Deployment Scenarios Edge deployment Cloud deployment","title":"Deployment Scenarios"},{"location":"developers/deployment/#deployment-scenarios","text":"Edge deployment Cloud deployment","title":"Deployment Scenarios"},{"location":"developers/edge-deployment/","text":"Edge IoT Deployment Scenarios Raspberry Pi Minimal install, capacity planning, scaling, monitoring.","title":"Edge IoT Deployment Scenarios"},{"location":"developers/edge-deployment/#edge-iot-deployment-scenarios","text":"Raspberry Pi Minimal install, capacity planning, scaling, monitoring.","title":"Edge IoT Deployment Scenarios"},{"location":"developers/hass-integration/","text":"Home Assistant Integration TBD","title":"Home Assistant Integration"},{"location":"developers/hass-integration/#home-assistant-integration","text":"TBD","title":"Home Assistant Integration"},{"location":"developers/home-bridge-integration/","text":"Home Bridge Integration TBD","title":"Home Bridge Integration"},{"location":"developers/home-bridge-integration/#home-bridge-integration","text":"TBD","title":"Home Bridge Integration"},{"location":"developers/integration/","text":"Integration with Smart Home hubs Home Assistant More on Home Assistant Integration Home Bridge More on Home Bridge Integration","title":"Integrations"},{"location":"developers/integration/#integration-with-smart-home-hubs","text":"","title":"Integration with Smart Home hubs"},{"location":"developers/integration/#home-assistant","text":"More on Home Assistant Integration","title":"Home Assistant"},{"location":"developers/integration/#home-bridge","text":"More on Home Bridge Integration","title":"Home Bridge"},{"location":"developers/js-api/","text":"Frontend JavaScript API TBD","title":"Frontend JavaScript API"},{"location":"developers/js-api/#frontend-javascript-api","text":"TBD","title":"Frontend JavaScript API"},{"location":"developers/mqtt-api/","text":"MQTT API TBD","title":"MQTT API"},{"location":"developers/mqtt-api/#mqtt-api","text":"TBD","title":"MQTT API"},{"location":"developers/publish-bazaar/","text":"Publishing to AI Apps Bazaar TBD","title":"Publishing to AI Apps Bazaar"},{"location":"developers/publish-bazaar/#publishing-to-ai-apps-bazaar","text":"TBD","title":"Publishing to AI Apps Bazaar"},{"location":"developers/python-api/","text":"Backend Python API TBD High level description link to raw API","title":"Backend Python API"},{"location":"developers/python-api/#backend-python-api","text":"TBD High level description link to raw API","title":"Backend Python API"},{"location":"developers/raw-python-api/","text":"ambianic ambianic.pipeline Main module for Ambianic AI pipelines. PipeElement PipeElement(self) The basic building block of an Ambianic pipeline. healthcheck PipeElement.healthcheck(self) Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally). heartbeat PipeElement.heartbeat(self) Set the heartbeat timestamp to time.monotonic(). stop PipeElement.stop(self) Receive stop signal and act accordingly. Subclasses should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing. connect_to_next_element PipeElement.connect_to_next_element(self, next_element=None) Connect this element to the next element in the pipe. Subclasses should not have to override this method. receive_next_sample PipeElement.receive_next_sample(self, **sample) Receive next sample from a connected previous element. Subclasses should not have to override this method. :Parameters: **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. process_sample PipeElement.process_sample(self, **sample) Implement processing in subclass as a generator function. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline. :Parameters: **sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: processed_sample: dict Processed sample that will be passed to the next pipeline element. HealthChecker HealthChecker(self, health_status_callback=None) Monitor overall pipeline throughput health. Attaches at the end of a pipeline to monitor its health status based on received output samples and their frequency. process_sample HealthChecker.process_sample(self, **sample) Call health callback and pass on sample as is. ambianic.pipeline.ai ambianic.pipeline.avsource ambianic.webapp","title":"ambianic"},{"location":"developers/raw-python-api/#ambianic","text":"","title":"ambianic"},{"location":"developers/raw-python-api/#ambianicpipeline","text":"Main module for Ambianic AI pipelines.","title":"ambianic.pipeline"},{"location":"developers/raw-python-api/#pipeelement","text":"PipeElement(self) The basic building block of an Ambianic pipeline.","title":"PipeElement"},{"location":"developers/raw-python-api/#healthcheck","text":"PipeElement.healthcheck(self) Check the health of this element. :returns: (timestamp, status) tuple with most recent heartbeat timestamp and health status code ('OK' normally).","title":"healthcheck"},{"location":"developers/raw-python-api/#heartbeat","text":"PipeElement.heartbeat(self) Set the heartbeat timestamp to time.monotonic().","title":"heartbeat"},{"location":"developers/raw-python-api/#stop","text":"PipeElement.stop(self) Receive stop signal and act accordingly. Subclasses should override this method by first invoking their super class implementation and then running through steps specific to stopping their ongoing sample processing.","title":"stop"},{"location":"developers/raw-python-api/#connect_to_next_element","text":"PipeElement.connect_to_next_element(self, next_element=None) Connect this element to the next element in the pipe. Subclasses should not have to override this method.","title":"connect_to_next_element"},{"location":"developers/raw-python-api/#receive_next_sample","text":"PipeElement.receive_next_sample(self, **sample) Receive next sample from a connected previous element. Subclasses should not have to override this method.","title":"receive_next_sample"},{"location":"developers/raw-python-api/#parameters","text":"**sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements.","title":":Parameters:"},{"location":"developers/raw-python-api/#process_sample","text":"PipeElement.process_sample(self, **sample) Implement processing in subclass as a generator function. Invoked by receive_next_sample() when the previous element (or pipeline source) feeds another data input sample. Implementing subclasses should process input samples and yield output samples for the next element in the pipeline.","title":"process_sample"},{"location":"developers/raw-python-api/#parameters_1","text":"**sample : dict A dict of (key, value) pairs that represent the sample. It is left to specialized implementations of PipeElement to specify their in/out sample formats and enforce compatibility with adjacent connected pipe elements. :Returns: processed_sample: dict Processed sample that will be passed to the next pipeline element.","title":":Parameters:"},{"location":"developers/raw-python-api/#healthchecker","text":"HealthChecker(self, health_status_callback=None) Monitor overall pipeline throughput health. Attaches at the end of a pipeline to monitor its health status based on received output samples and their frequency.","title":"HealthChecker"},{"location":"developers/raw-python-api/#process_sample_1","text":"HealthChecker.process_sample(self, **sample) Call health callback and pass on sample as is.","title":"process_sample"},{"location":"developers/raw-python-api/#ambianicpipelineai","text":"","title":"ambianic.pipeline.ai"},{"location":"developers/raw-python-api/#ambianicpipelineavsource","text":"","title":"ambianic.pipeline.avsource"},{"location":"developers/raw-python-api/#ambianicwebapp","text":"","title":"ambianic.webapp"},{"location":"developers/rest-api/","text":"REST API TBD","title":"REST API"},{"location":"developers/rest-api/#rest-api","text":"TBD","title":"REST API"},{"location":"developers/testing-ai-apps/","text":"Testing AI Apps TBD","title":"Testing AI Apps"},{"location":"developers/testing-ai-apps/#testing-ai-apps","text":"TBD","title":"Testing AI Apps"},{"location":"developers/websockets-api/","text":"WebSockets API TBD","title":"WebSockets API"},{"location":"developers/websockets-api/#websockets-api","text":"TBD","title":"WebSockets API"},{"location":"developers/writing-ai-app-overview/","text":"How to write and publish your own AI app Writing Custom AI apps More about Writing your first AI App Testing Custom AI apps More about Testing your AI app Publishing to App Bazaar More about Publishing your AI app","title":"Writing AI Apps"},{"location":"developers/writing-ai-app-overview/#how-to-write-and-publish-your-own-ai-app","text":"","title":"How to write and publish your own AI app"},{"location":"developers/writing-ai-app-overview/#writing-custom-ai-apps","text":"More about Writing your first AI App","title":"Writing Custom AI apps"},{"location":"developers/writing-ai-app-overview/#testing-custom-ai-apps","text":"More about Testing your AI app","title":"Testing Custom AI apps"},{"location":"developers/writing-ai-app-overview/#publishing-to-app-bazaar","text":"More about Publishing your AI app","title":"Publishing to App Bazaar"},{"location":"developers/writing-ai-apps/","text":"Writing AI Apps TBD","title":"Writing AI Apps"},{"location":"developers/writing-ai-apps/#writing-ai-apps","text":"TBD","title":"Writing AI Apps"},{"location":"legal/CONTRIBUTING/","text":"How to Contribute We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow. Contributor License Agreement Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://ambianic.ai/cla to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again. Code reviews All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests. Community Guidelines This project follows Google's Open Source Community Guidelines .","title":"Contributing"},{"location":"legal/CONTRIBUTING/#how-to-contribute","text":"We'd love to accept your patches and contributions to this project. There are just a few small guidelines you need to follow.","title":"How to Contribute"},{"location":"legal/CONTRIBUTING/#contributor-license-agreement","text":"Contributions to this project must be accompanied by a Contributor License Agreement. You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project. Head over to https://ambianic.ai/cla to see your current agreements on file or to sign a new one. You generally only need to submit a CLA once, so if you've already submitted one (even if it was for a different project), you probably don't need to do it again.","title":"Contributor License Agreement"},{"location":"legal/CONTRIBUTING/#code-reviews","text":"All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose. Consult GitHub Help for more information on using pull requests.","title":"Code reviews"},{"location":"legal/CONTRIBUTING/#community-guidelines","text":"This project follows Google's Open Source Community Guidelines .","title":"Community Guidelines"},{"location":"users/ai-apps-bazaar/","text":"About the AI Apps Bazaar Github Repo URL Docker hub URL?","title":"About the AI Apps Bazaar"},{"location":"users/ai-apps-bazaar/#about-the-ai-apps-bazaar","text":"Github Repo URL Docker hub URL?","title":"About the AI Apps Bazaar"},{"location":"users/ai-apps/","text":"Ambianic AI Apps Standard apps Custom apps Apps Bazaar","title":"AI Apps"},{"location":"users/ai-apps/#ambianic-ai-apps","text":"Standard apps Custom apps Apps Bazaar","title":"Ambianic AI Apps"},{"location":"users/configure/","text":"Configuring Ambianic Quickstart reference Most common configuration settings Advanced configuration Configuration changes: runtime safe and restart changes st=>start: Start|past:>http://www.google.com[blank] e=>end: Ende|future:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|future st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e $(\".diagram\").flowchart();","title":"Configuring"},{"location":"users/configure/#configuring-ambianic","text":"Quickstart reference Most common configuration settings Advanced configuration Configuration changes: runtime safe and restart changes st=>start: Start|past:>http://www.google.com[blank] e=>end: Ende|future:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|future st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e $(\".diagram\").flowchart();","title":"Configuring Ambianic"},{"location":"users/custom-ai-apps/","text":"Installing and Running Custom AI Apps Examples of custom apps Where to get custom apps How to install and run","title":"Installing and Running Custom AI Apps"},{"location":"users/custom-ai-apps/#installing-and-running-custom-ai-apps","text":"Examples of custom apps Where to get custom apps How to install and run","title":"Installing and Running Custom AI Apps"},{"location":"users/faq/","text":"Frequently Asked Questions Does Ambianic use hardware accelerators for AI inference? Yes. Ambianic currently supports the Google Coral EdgeTPU. The server relies on the Tensorflow Lite Runtime to dynamically detect Coral. If no EdgeTPU is available, AI inference falls back on the CPU. Check your logs for messages indicating EdgeTPU availability. Does Ambianic use hardware accelerators for video processing? Yes. Ambianic relies on gstreamer to dynamically detect and use any available GPU on the host platform for video and image processing. Asked Asked Asked 6. 2. TBD","title":"FAQ"},{"location":"users/faq/#frequently-asked-questions","text":"Does Ambianic use hardware accelerators for AI inference? Yes. Ambianic currently supports the Google Coral EdgeTPU. The server relies on the Tensorflow Lite Runtime to dynamically detect Coral. If no EdgeTPU is available, AI inference falls back on the CPU. Check your logs for messages indicating EdgeTPU availability. Does Ambianic use hardware accelerators for video processing? Yes. Ambianic relies on gstreamer to dynamically detect and use any available GPU on the host platform for video and image processing. Asked Asked Asked 6. 2. TBD","title":"Frequently Asked Questions"},{"location":"users/howto/","text":"How To TBD","title":"How To"},{"location":"users/howto/#how-to","text":"TBD","title":"How To"},{"location":"users/install/","text":"Installing and Running Ambianic Quickstart reference Various install scenarios: CPU, OS matrix","title":"Installing"},{"location":"users/install/#installing-and-running-ambianic","text":"Quickstart reference Various install scenarios: CPU, OS matrix","title":"Installing and Running Ambianic"},{"location":"users/mobileui/","text":"Amianic Mobile UI Registration and login Your Inference Timeline Alerts and Notifications","title":"Mobile UI"},{"location":"users/mobileui/#amianic-mobile-ui","text":"Registration and login Your Inference Timeline Alerts and Notifications","title":"Amianic Mobile UI"},{"location":"users/quickstart/","text":"Quick Start Ambianic's main goal is to provide helpful suggestions in the context of home and business automation. The main unit of work is the Ambianic pipeline. The following diagram illustrates an example pipeline that takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart(); Configuration Here is the corresponding configuration section in config.yaml for the pipeline above: pipelines: # sequence of piped operations for use in daytime front door watch daytime_front_door_watch: - source: *src_front_door_cam - detect_objects: # run ai inference on the input data <<: *tfm_image_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: *object_detect_dir positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output <<: *tfm_face_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: *face_detect_dir positive_interval: 2 idle_interval: 600 In the configuration excerpt above, there are a few references to variables defined elsewhere in the YAML file. *src_front_door_cam is the only reference that you have to understand and configure in order to get Ambianic working with your own camera (or other source of video feed). Here is the definition of this variable reference that you will find in config.yaml: sources: front_door_camera: &src_front_door_cam uri: *secret_uri_front_door_camera type: video The key parameter here is uri . We recommended that you store the value in secrets.yaml which needs to be located in the same directory as config.yaml . A valid entry in secretes.yaml for a camera URI, would look like this: secret_uri_front_door_camera: &secret_uri_front_door_camera 'rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101' # add more secret entries as regular yaml mappings Assuming you are familiar with yaml syntax, the rest of the configuration settings can be left with their default values. Once you specify the URI of your camera, you can navigate to the Ambianic working directory and start the server with: ./ambianic-start.sh You can find object and face detections stored in the ./data directory by default. Respectively under ./data/objects and ./data/faces .","title":"Quick Start"},{"location":"users/quickstart/#quick-start","text":"Ambianic's main goal is to provide helpful suggestions in the context of home and business automation. The main unit of work is the Ambianic pipeline. The following diagram illustrates an example pipeline that takes as input a video stream URI source such as a surveillance camera and outputs object detections to a local directory. st=>start: Video Source op_obj=>operation: Object Detection AI op_sav1=>parallel: Storage Element io1=>inputoutput: save object detections to file op_face=>operation: Face Detection AI op_sav2=>parallel: Storage Element io2=>inputoutput: save face detections to file e=>end: Output to other pipelines st->op_obj op_obj(bottom)->op_sav1 op_sav1(path1, bottom)->op_face op_sav1(path2, right)->io1 op_face(bottom)->op_sav2 op_sav2(path1, bottom)->e op_sav2(path2, right)->io2 $(\".diagram\").flowchart();","title":"Quick Start"},{"location":"users/quickstart/#configuration","text":"Here is the corresponding configuration section in config.yaml for the pipeline above: pipelines: # sequence of piped operations for use in daytime front door watch daytime_front_door_watch: - source: *src_front_door_cam - detect_objects: # run ai inference on the input data <<: *tfm_image_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: *object_detect_dir positive_interval: 2 # how often (in seconds) to save samples with ANY results above the confidence threshold idle_interval: 6000 # how often (in seconds) to save samples with NO results above the confidence threshold - detect_faces: # run ai inference on the samples from the previous element output <<: *tfm_face_detection confidence_threshold: 0.8 - save_detections: # save samples from the inference results output_directory: *face_detect_dir positive_interval: 2 idle_interval: 600 In the configuration excerpt above, there are a few references to variables defined elsewhere in the YAML file. *src_front_door_cam is the only reference that you have to understand and configure in order to get Ambianic working with your own camera (or other source of video feed). Here is the definition of this variable reference that you will find in config.yaml: sources: front_door_camera: &src_front_door_cam uri: *secret_uri_front_door_camera type: video The key parameter here is uri . We recommended that you store the value in secrets.yaml which needs to be located in the same directory as config.yaml . A valid entry in secretes.yaml for a camera URI, would look like this: secret_uri_front_door_camera: &secret_uri_front_door_camera 'rtsp://user:pass@192.168.86.111:554/Streaming/Channels/101' # add more secret entries as regular yaml mappings Assuming you are familiar with yaml syntax, the rest of the configuration settings can be left with their default values. Once you specify the URI of your camera, you can navigate to the Ambianic working directory and start the server with: ./ambianic-start.sh You can find object and face detections stored in the ./data directory by default. Respectively under ./data/objects and ./data/faces .","title":"Configuration"},{"location":"users/standard-ai-apps/","text":"Standard AI Apps Included with the base Ambianic distribution Object Detection Face Detection Face Recognition Etc.","title":"Standard AI Apps Included with the base Ambianic distribution"},{"location":"users/standard-ai-apps/#standard-ai-apps-included-with-the-base-ambianic-distribution","text":"Object Detection Face Detection Face Recognition Etc.","title":"Standard AI Apps Included with the base Ambianic distribution"},{"location":"users/support/","text":"Support TBD","title":"Support"},{"location":"users/support/#support","text":"TBD","title":"Support"},{"location":"users/webui/","text":"Amianic Web UI Registration and login Securing access AI Apps Overview Pipelines Overview Advanced Pipeline Composition","title":"Web UI"},{"location":"users/webui/#amianic-web-ui","text":"Registration and login Securing access AI Apps Overview Pipelines Overview Advanced Pipeline Composition","title":"Amianic Web UI"}]}